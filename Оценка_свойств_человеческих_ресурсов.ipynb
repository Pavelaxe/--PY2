{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef935e6a",
   "metadata": {},
   "source": [
    "# Парсинг Elibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "import pymorphy2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b806b67",
   "metadata": {},
   "source": [
    "Предварительно необходимо создать подборки по преподавателям на Elibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_teachers = {'Вилькен_Виктория_Валерьевна' : 2, 'Ливинцова_Мария_Геннадьевна' : 3,\n",
    "                    'Климин_Анастасий_Игоревич' : 4, 'Лукашевич_Никита_Сергеевич' : 5,\n",
    "                    'Абушова_Екатерина_Евгеньевна' : 6, 'Кальченко_Ольга_Александровна' : 7,\n",
    "                    'Тихонов_Дмитрий_Владимирович' : 8, 'Дуболазов_Анатолий_Александрович' : 9,\n",
    "                    'Симакова_Зоя_Леонидовна' : 10, 'Авдуевская_Екатерина_Алексеевна' : 11,\n",
    "                    'Богданова_Татьяна_Александровна' : 12, 'Фадеев_Алексей_Михайлович' : 13,\n",
    "                    'Бабкин_Иван_Александрович' : 14, 'Пупенцова_Светлана_Валентиновна' : 15,\n",
    "                    'Гутман_Светлана_Семеновна' : 16, 'Дмитриев_Николай_Дмитриевич' : 17,\n",
    "                    'Дуболазова_Юлия_Андреевна' : 18, 'Кожина_Ксения_Сергеевна' : 19,\n",
    "                    'Конников_Евгений_Александрович' : 20, \n",
    "                    'Муханова_Наталья_Викторовна' : 22, 'Некрасова_Татьяна_Петровна' : 23,\n",
    "                    'Пашоликов_Максим_Александрович' : 24, 'Покровская_Любовь_Леонидовна' : 25,\n",
    "                    'Рудская_Ирина_Андреевна' : 26, 'Схведиани_Анги_Ерастиевич' : 27,\n",
    "                    'Томшинская_Ирина_Николаевна' : 28, 'Левенцов_Валерий_Александрович': 29}\n",
    "\n",
    "password = \"Ваш_пароль\"\n",
    "login = \"Ваш_логин\"\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless\")\n",
    "\n",
    "options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "driver = webdriver.Chrome(executable_path='chromedriver', chrome_options = options)\n",
    "\n",
    "driver.implicitly_wait(2)\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "driver.get('https://www.elibrary.ru/itembox_items.asp?id=1369511')\n",
    "time.sleep(random.uniform(0, 2))\n",
    "\n",
    "WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '/html/body/table/tbody/tr/td/table/tbody/tr/td[1]/table/tbody/tr[6]/td/div/table/tbody/tr/td/table[1]/tbody/tr[6]/td/input'))).send_keys(login)\n",
    "WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '/html/body/table/tbody/tr/td/table/tbody/tr/td[1]/table/tbody/tr[6]/td/div/table/tbody/tr/td/table[1]/tbody/tr[8]/td/input'))).send_keys(password)\n",
    "\n",
    "WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, '/html/body/table/tbody/tr/td/table/tbody/tr/td[1]/table/tbody/tr[6]/td/div/table/tbody/tr/td/table[1]/tbody/tr[9]/td/input'))).click()\n",
    "\n",
    "for k, v in dict_of_teachers.items():\n",
    "    WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr[3]/td/form/table/tbody/tr/td[2]/table/tbody/tr[{v}]/td[2]/font/a'))).click()                                  \n",
    "    time.sleep(random.uniform(1, 5))\n",
    "    test = driver.page_source\n",
    "    \n",
    "    index_for_pages = 4\n",
    "    quantity_of_back = 1\n",
    "\n",
    "    quantity_of_articles = len(BeautifulSoup(test).findAll('span', {'style' : 'line-height:1.0;'}))\n",
    "\n",
    "    inf_about_requests = []\n",
    "\n",
    "    for i in tqdm(range(2, quantity_of_articles + 2)):\n",
    "        try:\n",
    "            try:\n",
    "                try:\n",
    "                    WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/div[3]/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr[3]/td/form/table[2]/tbody/tr[{i}]/td[2]/a/b/span'))).click()\n",
    "                except:\n",
    "                    WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/div[2]/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr[3]/td/form/table[2]/tbody/tr[{i}]/td[2]/a/b/span'))).click()\n",
    "            except:\n",
    "                driver.back()\n",
    "                WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/div[3]/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr[3]/td/form/table[2]/tbody/tr[{i}]/td[2]/a/b/span'))).click()\n",
    "                WebDriverWait(driver, 40).until(EC.element_to_be_clickable((By.XPATH, '/html/body/table/tbody/tr/td/table[1]/tbody/tr/td[2]/table/tbody/tr[2]/td[1]/table[1]/tbody/tr/td[2]/font/a')))\n",
    "        except:\n",
    "            driver.back()\n",
    "            WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/div[3]/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr[3]/td/form/table[2]/tbody/tr[{i}]/td[2]/a/b/span'))).click()\n",
    "        try:\n",
    "            WebDriverWait(driver, 1).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/table/tbody/tr/td/table[1]/tbody/tr/td[2]/table/tbody/tr[2]/td[1]/div/table[5]/tbody/tr[2]/td[2]/div[3]/a'))).click()\n",
    "        except:\n",
    "            time.sleep(0.1)\n",
    "        time.sleep(random.uniform(0, 2))\n",
    "        WebDriverWait(driver, 40).until(EC.element_to_be_clickable((By.XPATH, '/html/body/table/tbody/tr/td/table[1]/tbody/tr/td[2]/table/tbody/tr[2]/td[1]/table[1]/tbody/tr/td[2]/font/a')))\n",
    "        inf_about_requests.append(driver.page_source)\n",
    "        driver.back()\n",
    "        time.sleep(random.uniform(0, 2))\n",
    "        \n",
    "    while quantity_of_articles == 100:\n",
    "        WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/div[3]/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr[3]/td/form/table[3]/tbody/tr/td[{index_for_pages}]/a'))).click()\n",
    "        time.sleep(2)\n",
    "        test = driver.page_source\n",
    "        quantity_of_articles = len(BeautifulSoup(test).findAll('span', {'style' : 'line-height:1.0;'}))                                                           \n",
    "        for i in tqdm(range(2, quantity_of_articles + 2)):\n",
    "            try:\n",
    "                try:\n",
    "                    try:\n",
    "                        WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/div[3]/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr[3]/td/form/table[2]/tbody/tr[{i}]/td[2]/a/b/span'))).click()\n",
    "                    except:\n",
    "                        WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/div[2]/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr[3]/td/form/table[2]/tbody/tr[{i}]/td[2]/a/b/span'))).click()\n",
    "                except:\n",
    "                    driver.back()\n",
    "                    WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/div[3]/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr[3]/td/form/table[2]/tbody/tr[{i}]/td[2]/a/b/span'))).click()\n",
    "                    WebDriverWait(driver, 40).until(EC.element_to_be_clickable((By.XPATH, '/html/body/table/tbody/tr/td/table[1]/tbody/tr/td[2]/table/tbody/tr[2]/td[1]/table[1]/tbody/tr/td[2]/font/a')))\n",
    "            except:\n",
    "                driver.back()\n",
    "                WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/div[3]/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr[3]/td/form/table[2]/tbody/tr[{i}]/td[2]/a/b/span'))).click()\n",
    "            try:\n",
    "                WebDriverWait(driver, 1).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/table/tbody/tr/td/table[1]/tbody/tr/td[2]/table/tbody/tr[2]/td[1]/div/table[5]/tbody/tr[2]/td[2]/div[3]/a'))).click()\n",
    "            except:\n",
    "                time.sleep(0.1)\n",
    "            time.sleep(random.uniform(0, 2))\n",
    "            WebDriverWait(driver, 40).until(EC.element_to_be_clickable((By.XPATH, '/html/body/table/tbody/tr/td/table[1]/tbody/tr/td[2]/table/tbody/tr[2]/td[1]/table[1]/tbody/tr/td[2]/font/a')))\n",
    "            inf_about_requests.append(driver.page_source)\n",
    "            driver.back()\n",
    "            time.sleep(random.uniform(0, 2))\n",
    "        index_for_pages += 1\n",
    "        quantity_of_back += 1\n",
    "    \n",
    "    TITLE = []\n",
    "    AUTHORS = []\n",
    "    ABSTRACT = []\n",
    "    UNIVETSITY = []\n",
    "    SOURCE = []\n",
    "\n",
    "    for x in tqdm(inf_about_requests):\n",
    "        TITLE.append(BeautifulSoup(x).find('p', {'class' : 'bigtext'}).text)\n",
    "        oz_1 = []\n",
    "        oz = []\n",
    "        m = BeautifulSoup(x).findAll('span', {'class' : 'help pointer'})\n",
    "        for i in m:\n",
    "            if '1' in i.text:\n",
    "                new = re.sub('1', '', i.text)\n",
    "                oz.append(new)\n",
    "            else:\n",
    "                oz.append(i.text)\n",
    "        authors = ' '.join(oz)\n",
    "        AUTHORS.append(authors)\n",
    "        try:\n",
    "            ABSTRACT.append(BeautifulSoup(x).find('p', {'align' : 'justify'}).text)\n",
    "        except:\n",
    "            ABSTRACT.append('-')\n",
    "        university_1 = []\n",
    "        m = BeautifulSoup(x).findAll('span', {'class' : 'help1 pointer'})\n",
    "        for i in m:\n",
    "            university_1.append(i.text)\n",
    "        new = ' '.join(university_1)\n",
    "        UNIVETSITY.append(new)\n",
    "        m = BeautifulSoup(x).findAll('td', {'width' : '504'})\n",
    "        oz = []\n",
    "        for i in m:\n",
    "            oz.append(i.text)\n",
    "            source = ' '.join(oz)\n",
    "        SOURCE.append(source)\n",
    "\n",
    "    print(len(TITLE))\n",
    "    print(len(AUTHORS))\n",
    "    print(len(ABSTRACT))\n",
    "    print(len(UNIVETSITY))\n",
    "    print(len(SOURCE))\n",
    "\n",
    "    full_list = list(zip(TITLE, AUTHORS, ABSTRACT, UNIVETSITY, SOURCE))\n",
    "    df = pd.DataFrame(full_list, columns= ['Название статьи', 'Авторы', 'Аннотация', 'Организация', 'Источник'])\n",
    "    df.to_excel(f'{k}.xlsx')\n",
    "    for i in range(quantity_of_back):\n",
    "        driver.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea33cdb",
   "metadata": {},
   "source": [
    "Фильтрация собранных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "substrings = ['Санкт-Петербургский политехнический',\n",
    "              'Peter the Great St',\n",
    "              'St. Petersburg Polytechnic',\n",
    "              'St. Petersburg State',\n",
    "              'СПбГПУ',\n",
    "              'Санкт-Петербургский государственный политехнический',\n",
    "              'Saint-Petersburg State Polytechnical University',\n",
    "              'Peter the Great Saint',\n",
    "              'СПбПУ', 'Институт экономических проблем',\n",
    "              'Институт промышленного менеджмента', 'Institute of industrial management']\n",
    "\n",
    "substrings_1 = ['НЕДЕЛЯ НАУКИ СПБПУ', 'Издательство Политехнического университета',\n",
    "                'Санкт-Петербургский политехнический университет Петра Великого', 'ПОЛИТЕХ-ПРЕСС']\n",
    "\n",
    "regex_pattern = '|'.join(map(re.escape, substrings))\n",
    "regex_pattern_1 = '|'.join(map(re.escape, substrings_1))\n",
    "\n",
    "for sci in tqdm(list(dict_of_teachers.keys())):\n",
    "    df = pd.read_excel(f'{sci}.xlsx')\n",
    "    filt = (df['Организация'].str.contains(regex_pattern, case=False) | df['Источник'].str.contains(regex_pattern_1, case=False))\n",
    "    df[filt].to_excel(f'{sci}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c30b2",
   "metadata": {},
   "source": [
    "# Парсинг групп ВК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from tqdm.notebook import tqdm \n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0864e52",
   "metadata": {},
   "source": [
    "Предварительно необходимо узнать id страниц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640cff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_teachers = {'Абушова_Екатерина_Евгеньевна' : '213551',\n",
    "                   'Авдуевская_Екатерина_Алексеевна' : '87209380',\n",
    "                   'Бабкин_Иван_Александрович' : '93936875',\n",
    "                   'Богданова_Татьяна_Александровна' : '1934831',\n",
    "                   'Вилькен_Виктория_Валерьевна' : '27427904',\n",
    "                   'Гутман_Светлана_Семеновна' : '1492436',\n",
    "                   'Дмитриев_Николай_Дмитриевич' : '79214648',\n",
    "                   'Дуболазов_Анатолий_Александрович' : '289926',\n",
    "                   'Дуболазова_Юлия_Андреевна' : '17902599',\n",
    "                   'Кальченко_Ольга_Александровна' : '49222',\n",
    "                   'Климин_Анастасий_Игоревич' : '3396625',\n",
    "                   'Кожина_Ксения_Сергеевна' : '132657764',\n",
    "                   'Конников_Евгений_Александрович' : '1083727',\n",
    "                   'Левенцов_Валерий_Александрович' : '152326',\n",
    "                   'Ливинцова_Мария_Геннадьевна' : '1029537',\n",
    "                   'Лукашевич_Никита_Сергеевич' : '35147',\n",
    "                   'Муханова_Наталья_Викторовна' : '4210429',\n",
    "                   'Некрасова_Татьяна_Петровна' : '1425316',\n",
    "                   'Пашоликов_Максим_Александрович' : '5648',\n",
    "                   'Покровская_Любовь_Леонидовна' : '8542953',\n",
    "                   'Пупенцова_Светлана_Валентиновна' : '1885157',\n",
    "                   'Рудская_Ирина_Андреевна' : '89418228',\n",
    "                   'Симакова_Зоя_Леонидовна' : '1652858',\n",
    "                   'Схведиани_Анги_Ерастиевич' : '31369407',\n",
    "                   'Тихонов_Дмитрий_Владимирович' : '4550931',\n",
    "                   'Томшинская_Ирина_Николаевна' : '37281427',\n",
    "                   'Фадеев_Алексей_Михайлович' : '18914447'}\n",
    "\n",
    "url = 'https://api.vk.com/method/groups.get'\n",
    "\n",
    "params = {'access_token':'Ваш_ключ_доступа', \n",
    "          'v': '5.131',\n",
    "          'extended' : '1',\n",
    "          'fields': 'description'}\n",
    "\n",
    "for k, v in tqdm(dict_of_teachers.items()):\n",
    "    params['user_id'] = v\n",
    "    inf = requests.get(url, params = params).json()\n",
    "    description = []\n",
    "    for group in range(len(inf['response']['items'])):\n",
    "        try:\n",
    "            description.append(inf['response']['items'][group]['description'])\n",
    "        except:\n",
    "            description.append('-')\n",
    "    df = pd.DataFrame(description, columns = ['Описание групп'])\n",
    "    df.to_excel(f'Описание_групп_{k}.xlsx', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e06b37",
   "metadata": {},
   "source": [
    "# Векторизация и целевой коэффициент сходства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef7f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pymorphy2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cadf85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_vectors = []\n",
    "morf = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "for sci in tqdm(list(dict_of_teachers.keys())):\n",
    "    df = pd.read_excel(f'{sci}.xlsx')\n",
    "    texts = list(df['Название статьи']) + list(df['Аннотация']) \n",
    "    tokkens_lists = []\n",
    "    for sentence in texts:\n",
    "        tokkens_lists.extend(nltk.word_tokenize(sentence))\n",
    "    sent_teg_list = ['NOUN', 'ADJF', 'ADJS', 'VERB', 'INFN', 'ADVB']\n",
    "    tokens_1 = []\n",
    "    for token in tokkens_lists:\n",
    "        token_morf_inf = morf.parse(token)\n",
    "        if token_morf_inf[0].tag.POS in sent_teg_list:\n",
    "            tokens_1.append(token_morf_inf[0].normal_form)\n",
    "    vector = ' '.join(tokens_1)\n",
    "    list_of_vectors.append(vector)\n",
    "    \n",
    "tfidfconverter = TfidfVectorizer(max_features = 5000)\n",
    "\n",
    "X = tfidfconverter.fit_transform(list_of_vectors).toarray()\n",
    "    \n",
    "df_vect = pd.DataFrame(X)\n",
    "\n",
    "df_as_list = df_vect.apply(lambda row: row.tolist(), axis=1)\n",
    "vectors = list(df_as_list)\n",
    "vectors_array = np.array(vectors)\n",
    "norms = np.linalg.norm(vectors_array, axis=1)\n",
    "dot_products = np.dot(vectors_array, vectors_array.T)\n",
    "cosine_similarities = dot_products / np.outer(norms, norms)\n",
    "\n",
    "df_vect_1 = pd.DataFrame(cosine_similarities)\n",
    "df_vect_1.insert(0, ' ', list(dict_of_teachers.keys()))\n",
    "for i in range(len(dict_of_teachers)):\n",
    "    df_vect_1 = df_vect_1.rename(columns={f'{i}': f'{list(dict_of_teachers.keys())[i]}'})\n",
    "df_vect_1.columns = [[' '] + list(dict_of_teachers.keys())]\n",
    "\n",
    "df_vect_1.to_excel('Название_эксель_файла.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e176f",
   "metadata": {},
   "source": [
    "Точно такой же алгоритм для групп вк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b6127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_similarities_1 - матрица косинусного сходства по группам вк\n",
    "cosine_similarities_2 = cosine_similarities_1*0.5 + cosine_similarities*0.5\n",
    "df_vect_new_2 = pd.DataFrame(cosine_similarities_2)\n",
    "df_vect_new_2.insert(0, ' ', list(dict_of_teachers.keys()))\n",
    "for i in range(len(dict_of_teachers)):\n",
    "    df_vect_new_2 = df_vect_new_2.rename(columns={f'{i}': f'{list(dict_of_teachers.keys())[i]}'})\n",
    "df_vect_new_2.columns = [[' '] + list(dict_of_teachers.keys())]\n",
    "df_vect_new_2.to_excel('Коэфф_сходства.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
